# -*- coding: utf-8 -*-
"""SNN-Analysis(Basic).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mRFZIFjZAOiv2ygp-xxlNIC-Qq4e5bnH

#### Basic SNN
* Date : 25/06/2020
* For testing weight trajectory and plotting spike rastor plots

#### Usage

* Run all cells. 
* In upload from drive cell, give the folder id as per drive.
  * How to find folder id:
    * Example : Upload the required data to drive in a folder. 
    * Go into the folder, where all files are present.
    * The web address bar will have something as follows :
    'https://drive.google.com/drive/u/5/folders/1XpCOWPTLOKuLvc5VxuLA58L5Nmsh1Qd'

    Folder id is : 1XpCOWPTLOKuLvc5VxuLA58L5Nmsh1Qd

* Give the folder id in the first id to load all data.
* Run remaining cells
* The cell run() shall output the results.
* The plots can be zoomed and rotated and analysed.

* The cell plot_weight_traj plots the weight trajectory and plot_raster shall help in visualizing the spikes. 

#### References
* https://plotly.com/python/3d-surface-plots/
"""
## Uncomment if run on colab
## Install the dependencies which are not imbuilt in Google colab
# !pip install pyspike
# !pip install python_speech_features
# !pip install neuronpy
# !pip install pydriveimport numpy as np

import argparse
import numpy as np
import matplotlib.pyplot as plt
from random import shuffle
import os
import sys
from datetime import datetime
import copy
import math
import ntpath
from scipy.fftpack import fft
from scipy.signal import spectrogram
from scipy.io import wavfile
from python_speech_features.sigproc import framesig
import random as random
from matplotlib.pyplot import specgram
import wave
import contextlib
import zipfile
## Uncomment if run on colab
# from google.colab import files
# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials
import plotly.graph_objects as go
np.random.seed(1)

# 1. Authenticate and create the PyDrive client. This drive must contain the data (Uncomment if colab)
# auth.authenticate_user()
# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)
# print(drive)
# print("auth success")

class Utils:
    def get_features(file_name):
        (rate, sig) = wavfile.read(file_name)

        with contextlib.closing(wave.open(file_name, 'r')) as f:
            frames = f.getnframes()
            rate = f.getframerate()
            duration = frames / float(rate)

        # Frame our signal into 40 frames with 50% overlap
        number_of_frames = 40 ##changed from 40
        frame_len = len(sig) / (number_of_frames*(.5) + .5)
        frames = framesig(sig, frame_len, frame_len * .5)

        # A list of 40 frequency lists for each frame. 5 frequency bands with the average energy of each
        features = []
        band0 = []
        band1 = []
        band2 = []
        band3 = []
        band4 = []
        
        for frame in frames:
            spectrum, freqs, t, img = specgram(frame, Fs=rate)
            i = 0
            bands = []
            for freq in freqs:
                if freq <= 333.3:
                    band0.extend(spectrum[i])
                elif freq > 333.3 and freq <= 666.7:
                    band1.extend(spectrum[i])
                elif freq > 666.7 and freq <= 1333.3:
                    band2.extend(spectrum[i])
                elif freq > 1333.3 and freq <= 2333.3:
                    band3.extend(spectrum[i])
                elif freq > 2333.3 and freq <= 4000:
                    band4.extend(spectrum[i])

                i += 1
            bands.append(sum(band0) / len(band0))
            bands.append(sum(band1) / len(band1))
            bands.append(sum(band2) / len(band2))
            bands.append(sum(band3) / len(band3))
            bands.append(sum(band4) / len(band4))
            features.append(bands)
        
        values = []
        for feature in features:
            for f in feature:
                values.append(f)
        
        return values

   # Get label associated with this file
    def get_label(filename):
        
        head, tail = ntpath.split(filename)
        name = str(tail)
        label, rest1, rest2 = name.split('_')
        return label
      
    def plot_spikes(neuralData):
      
      colorCodes = np.array([[0,0,0,0],[0,1, 0, 0],[0,0, 1, 0],[0,0, 0, 1],[1,1,0,0],[1, 0, 1,0],[0, 1, 1,0],[0,1, 0, 1],[0,1,1,1],[1,1,1,1]])
      lineSize = [0.4, 0.3, 0.2, 0.8, 0.5, 0.6, 0.7, 0.9, 0.2, 0.5]                                  
      plt.eventplot(neuralData, color=colorCodes, linelengths = lineSize)  
      # Provide the title for the spike raster plot
      plt.title('Spike raster plot')
      # Give x axis label for the spike raster plot
      plt.xlabel('Neuron')
      # Give y axis label for the spike raster plot
      plt.ylabel('Spike')
      # Display the spike raster plot
      plt.show()

"""### Upload from drive"""
## Uncomment if run on colab
## Enter folder id
# folder_id = ''
# audio = drive.ListFile({'q': "'%s' in parents" % folder_id}).GetList()  

# mimetypes = {'application/vnd.google-apps.audio'}  
# for file1 in audio:
#   download_mimetype = None
#   if file1['mimeType'] in mimetypes:
#       download_mimetype = mimetypes[file1['mimeType']]

#   file1.GetContentFile(file1['title'], mimetype=download_mimetype)

# prototype_dict = {}
# for file in audio:
#     fname = file['title']
#     prototype_dict[fname] = Utils.get_label(fname)

class izhikevich:

    def __init__(self):
        # pre_syn and post_syn are the pre and post synapse times. synapses array hold the list of synapses associated with a neuron
        self.pre_syn = []
        self.post_syn = []
        self.synapses = []
        self.v_rest = -60
        self.v_threshold = -40
        self.v_peak =0  #35
        self.C = 100
        self.K = 0.7
        self.a=0.03
        self.b = -2
        self.c = -50
        self.d = 100


    def neuron_simulation(self, time_ita, current):
        # a,b,c,d parameters for Izhikevich model
        # time_ita time iterations for euler method
        # current list of current for each time step
        
        spike_times = []
        v = self.c     #v_init
        #u = v * self.b
        u = 0
        v_plt = np.zeros(time_ita)
        u_plt = np.zeros(time_ita)
        spike = np.zeros(time_ita)
        num_spikes = 0
        tstep = 0.1  # ms
        ita = 0
        while ita < time_ita:
            v_plt[ita] = v
            u_plt[ita] = u
            # dt = tstep
            # dU/dt = a * (b * (V - v_rest) - u)
            # dV/dt = (K/C)*(V-v_rest)*(V-v_threshold) - U/C + current[ita]/C
            v += tstep * ((self.K/self.C)*(v-self.v_rest)*(v-self.v_threshold)-u/self.C + current[ita]*100/self.C)
            u += tstep * self.a * (self.b * (v - self.v_rest) - u)
            if v > self.v_peak:
                spike[ita] = 1
                num_spikes += 1
                v = self.c
                u += self.d


            ita += 1
        time = np.arange(time_ita) * tstep
        i = 0
        for t in time:
            if spike[i] == 1:
                spike_times.append(t)
            i += 1
        return time, v_plt, spike, num_spikes, spike_times

    def append_pre_synapse_times(self, times):
        self.pre_syn = times

    def append_post_synapse_times(self, times):
        self.post_syn = times

    def append_synapse(self, synapse):
        self.synapses.append(synapse)

class Synapse:

    global spike, time, conductance_amplitude, w, spikes_received, pre_spikes, post_spikes, input_neuron, out_neuron

    def __init__(self):
        self.post_spikes = []
        self.pre_spikes = []
        self.w = random.uniform(0, 1)*0.01     ## Initial weights is set to 0.5 //ASSUMPTION!!!

    def set_weight(self, w):
        self.w = float(w)

    def set_time(self, time):
        self.time = time

    def set_spike(self, spike):
        self.spike = spike

    def set_pre_spikes(self, pre_spikes):
        self.pre_spikes = pre_spikes

    def set_post_spikes(self, post_spikes):
        self.post_spikes = post_spikes

    # Our W function for Hebbian STDP
    def heb_delta(self, delta_t):
        tau_pre = 2
        tau_post = 2
        Apre = .1
        Apost = -Apre
        if delta_t >= 0:
            return Apre*np.exp(-np.abs(delta_t)/tau_pre)
        if delta_t < 0:
            return Apost*np.exp(-np.abs(delta_t)/tau_post)

    # Our W function for anti-Hebbian STDP
    def anti_heb_delta(self, delta_t):
        tau_pre = 2
        tau_post = 2
        Apre = .1
        Apost = -Apre
        if delta_t < 0:
            return Apre*np.exp(-np.abs(delta_t)/tau_pre)
        if delta_t >= 0:
            return Apost*np.exp(-np.abs(delta_t)/tau_post)

    # Same as Hebbian STDP except the cases are reversed
    def Anti_Heb_STDP(self):
        delta_w = 0
        # for t_pre in self.pre_spikes:
        #     for t_post in self.post_spikes:
        #         delta_w += self.anti_heb_delta(t_post - t_pre)

        for i in range(len(self.post_spikes)):
          spike_post = self.post_spikes[i]
          if spike_post == 1:
            counter = 0 
            j = i
            while counter <20:
              pre  = self.pre_spikes[j] 
              if pre == 1:
                spike_pre = j
                j = -1
                break
              counter += 1
              if j == 0:
                j = len(self.post_spikes)-1
              else:
                j = j-1
          if j == -1:
             delta_t = (i-spike_pre)*0.1
             delta_w += self.anti_heb_delta(delta_t)
        
        self.w += delta_w
        if self.w <= 0:
            self. w = 0#random.uniform(0, 1)
 

    # Change in synaptic weight is the sum over all presynaptic spike times (t_pre) and postsynaptic spike times (t_post)
    # of some function W of the difference in these spike times
    def Heb_STDP(self):
        delta_w = 0
        
        # for t_pre in self.pre_spikes:
        #     for t_post in self.post_spikes:
        #         delta_w += self.heb_delta(t_post - t_pre)
        for i in range(len(self.post_spikes)):
          spike_post = self.post_spikes[i]
          j = 0
          if spike_post == 1:
            counter = 0 
            j = i
            while counter <20:
              pre  = self.pre_spikes[j] 
              if pre == 1:
                spike_pre = j
                j = -1
                break
              counter += 1
              if j == 0:
                j = len(self.post_spikes)-1
              else:
                j = j-1
          if j == -1:
             delta_t = (i-spike_pre)*0.1
             delta_w += self.anti_heb_delta(delta_t)

        self.w += delta_w
        if self.w <= 0:
            self.w = 0#random.uniform(0, 1)


    # Calculates synaptic output
    def synapse(self, tau):
        synapse_output = np.zeros(len(self.time))
        for t in range(len(self.time)):
            tmp_time = self.time[t] - self.time[0:t]
            synapse_output[t] = np.sum(((tmp_time * self.spike[0:t]) / tau) * np.exp(-(tmp_time * self.spike[0:t]) / tau))
        return self.w * synapse_output

    def synapse_func(self, tau):
        time = np.arange(10000) * 0.1
        func = time / tau * np.exp(-time / tau)
        return time, func

class Network:

    def __init__(self, weights):
        
        self.synapses = []
        self.weights_in_network = []
        self.time_ita = 2000  # 100ms
        # Build a layer of 200 input neurons 
        ## done for 100 neurons
        self.input_layer = []
        for i in range(200):
            n = izhikevich()
            self.input_layer.append(n)

        # Build output layer, one neuron for each digit 
        self.output_layer = []
        for i in range(1):
            o = izhikevich()
            self.output_layer.append(o)
        i = 0 
        # For each input neuron, append one synapse to each output neuron
        for n in self.input_layer:
            for out in self.output_layer:
                synapse = Synapse()
                n.append_synapse(synapse)
                out.append_synapse(synapse)
                self.synapses.append(synapse)

        if weights is not None:  # weights == None means the call from train()
            i = 0
            n = 0
            s = 0
            #with open(weights) as f:
            for line in weights:
                # For every 200 synapses, go to the next neuron
                if i % 200 == 0 and n < len(self.output_layer):
                    s = 0
                    out = self.output_layer[n]
                    n += 1
                    out.synapses[s].set_weight(line)
                    s += 1
                elif s < 200:
                    out.synapses[s].set_weight(line)
                    s += 1
                i += 1

    def save_weights_of_Network(self):
      arr = []
      count = 0
      for syn in self.synapses:
        w = syn.w
        arr.append(w)
        count+=1
      
      print(count)
      self.weights_in_network = arr
      print('Saved weights !')

    def normalize_weights(self):
      weights_sum = 0
      for syn in self.synapses:
        w = syn.w
        weights_sum+=float(w)

      for syn in self.synapses:
        w = syn.w
        w = float(w)/weights_sum
        syn.w = w
    
    # Calculates the current from the feature
    def get_current(self, x):
        return x
    """
    def get_total(self, neuron):
        g_tot = 0
        for synapse in neuron.synapses:
            t = 0
            i = 0
            tau = 2
            for j in synapse.spike:
                if j == 1:
                    t_kj = synapse.time[i]
                    t = np.abs(t - t_kj)
                    g_tot += synapse.w * (t - t_kj) * np.exp(-(t - t_kj) / tau)
                    t = t_kj
                i += 1
        return g_tot

    """

    # Get the total synaptic output for this neuron
    def total_synaptic_value(self, neuron):
        conductance = 0
        for syn_k in neuron.synapses:
            output = syn_k.synapse(2)
            conductance += output
        return conductance

    # if result == 0, then our target neuron is the first neuron in the output layer
    # result == 1 --> 2nd output neuron, result == 3 --> 3rd output neuron and so on
    def conduct_training(self, result):

      out1 = self.output_layer[0]
      if result == 0:
        for syn in out1.synapses:
          syn.Heb_STDP()
        
      else:
        for syn in out1.synapses:
          syn.Anti_Heb_STDP()

      # normalize_weights()
      print('done training')
      # self.save_weights_of_Network()

    def start(self, fname):

        features = Utils.get_features(fname)
        print('len of features', len(features))
        features = features[:200]
        
        #print('The input features', features)
        #Feed features into our network and get spike information (number of spikes, time of largest spike)
        i = 0
        print('Number of times steps', self.time_ita)
        # Use feature for 200 input neurons
        input_spikes = {}
        input_numSpikes = {}
        neuron_num = 0
        for feature in features:
            n = self.input_layer[i]
            current = np.ones(self.time_ita) * self.get_current(feature)
            #spike has 1 at time steps a spike occurred and 0 wherever no spike. 
            #spike_times is an array of the time steps when a spike occurred
            #v_plt is an array containing voltage value at each time step.
            # time is an array of the time steps
            time, v_plt, spike, num_spikes, spike_times = n.neuron_simulation(self.time_ita, current)
            input_spikes[neuron_num] = np.array(spike)
            input_numSpikes[neuron_num] = num_spikes
            neuron_num+=1
            # print('num_spikes pre', num_spikes)
            
            # Set pre spikes for each synapse connected to this neuron
            for synapse in n.synapses:
                #synapse.set_pre_spikes(spike_times)
                synapse.set_pre_spikes(spike)
                synapse.set_time(time)
                synapse.set_spike(spike)
            i += 1


        # Create a 10 neuron output vector
        outputs = [0] * 1
        spikes = []
        v_plts = []
        currents = []
        i = 0
        for out in self.output_layer:
            current = np.ones(self.time_ita) * self.total_synaptic_value(out)
            #print(' Input to output neuron', current)
            time, v_plt, spike, num_spikes, spike_times = out.neuron_simulation(self.time_ita, current)
            spikes.append(spike)
            v_plts.append(v_plt)
            currents.append(current)
            for syn in out.synapses:
                #syn.set_post_spikes(spike_times)
                syn.set_post_spikes(spike)

            outputs[i] = num_spikes
            i += 1

        return outputs, currents, time, v_plts, spikes,input_spikes, input_numSpikes

def run(prototype_dict):
  train_ = list(prototype_dict.keys())
  network = Network(None)
  weights_array = []

  num_of_epochs = 1
  print('Start time :', datetime.now())
  for j in range(num_of_epochs):
    for i in range(len(train_)):
      key = train_[i]
      dig = key.split('_')[0]
      results, currents, time, v_plts, spikes,input_spikes, input_numSpikes = network.start(key)
      print('Going to conduct training', str(dig))
      network.conduct_training(0)
      network.save_weights_of_Network()
      network.normalize_weights()
      weights = network.weights_in_network
      weights_array.append([weights])

  
  key = train_[0]
  results, currents, time, v_plts, output_spikes,input_spikes,input_numSpikes = network.start(key)
  print('end time :', datetime.now())

  weights_data = []
  for i in range(len(weights_array)):
    w = weights_array[i][0]
    weights_data.append(w)

  weights_data = np.array(weights_data)
  latest_weights = np.array(weights_data[-1])
  print('latest weights size', latest_weights.shape)

  indices = latest_weights.argsort()[-1:][::-1]
  rastor_plot_spikes = []

  for i in indices:
    s = input_spikes[i]
    rastor_plot_spikes.append(np.array(s))


  print('weights_data shape', weights_data.shape)

  sh_x = weights_data.shape[0]
  sh_y = weights_data.shape[1]

  weights_data_plot = weights_data.T
  print('weights_data_plot shape', weights_data_plot.shape)

  return weights_data_plot,rastor_plot_spikes,input_spikes, output_spikes, input_numSpikes

def plot_weight_traj(weights_data_plot):
  sh_x = weights_data_plot.shape[0]
  sh_y = weights_data_plot.shape[1]
  print(sh_x)
  print(sh_y)
  x, y = np.linspace(0, sh_y-1, sh_y), np.linspace(0, sh_x-1, sh_x)
  fig = go.Figure(data=[go.Surface(z=weights_data_plot, x=x, y=y)])
  fig.update_layout(title='Weight trajectory', autosize=False,
                    width=500, height=500,
                    margin=dict(l=65, r=50, b=65, t=90))
  fig.show()

def plot_raster(spike):

  x = np.arange(0,2000,1)
  y = spike
  plt.scatter(x,y, marker= 'x')
  plt.show()

def main(args):
    path = args.path
    audio = os.listdir(path)
    prototype_dict = {}

    for fname in audio:
        filename = path+'/'+fname
        prototype_dict[filename]  = Utils.get_label(filename)

    weights_data_plot,rastor_plot_spikes,input_spikes, output_spikes,input_numSpikes = run(prototype_dict)

    plot_weight_traj(weights_data_plot)

    for spike in rastor_plot_spikes:
        plot_raster(spike)

    for spike in output_spikes:
        plot_raster(spike)

if __name__ == "__main__":
	
	parser = argparse.ArgumentParser(description='Nissl Cell Segmentation')
	parser.add_argument('--path', default = '\home', type = str, help = 'path to the input files')
	args = parser.parse_args()
	main(args)


# /media/aiswarya/New Volume/My_works/SNN_works/my_code/spoken-digit-dataset/free-spoken-digit-dataset-master/trial_test
